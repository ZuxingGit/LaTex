
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkh{\"a}user" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }


% Added by me
@article{KoubaaAnis2025Nhiw,
abstract = {This article presents an innovative concept that harnesses the capabilities of large language models (LLMs) to revolutionize human‐robot interaction. This work aims to connect large language models with the Robot Operating System (ROS), the primary development framework for robotics applications. We develop a package for ROS that seamlessly integrates ChatGPT with ROS2‐based robotic systems. The core idea is to leverage prompt engineering with LLMs, utilizing unique properties such as ability eliciting, chain‐of‐thought, and instruction tuning. The concept employs ontology development to convert unstructured natural language commands into structured robotic instructions specific to the application context through prompt engineering. We capitalize on LLMs' zero‐shots and few‐shots learning capabilities by eliciting structured robotic commands from unstructured human language inputs. To demonstrate the feasibility of this concept, we implemented a proof‐of‐concept that integrates ChatGPT with ROS2, showcasing the transformation of human language instructions into spatial navigation commands for a ROS2‐enabled robot. Besides, we quantitatively evaluated this transformation over three use cases (ground robot, unmanned aerial vehicle, and Robotic arm) and five LLMs (LLaMA‐7b, LLaMA2‐7b, LLaMA2‐70b, GPT‐3.5, and GPT‐4) on a set of 3000 natural language commands. Our system serves as a new stride towards Artificial General Intelligence (AGI) and paves the way for the robotics and natural language processing communities to collaborate in creating novel, intuitive human‐robot interactions. The open‐source implementation of our system on ROS 2 is available on GitHub.},
author = {Koubaa, Anis and Ammar, Adel and Boulila, Wadii},
address = {Bognor Regis},
copyright = {2024 John Wiley & Sons Ltd.},
issn = {0038-0644},
journal = {Software, practice \& experience},
keywords = {Artificial intelligence ; Chatbots ; Drone aircraft ; Harnesses ; Language ; Mobile robots ; Natural Language Processing ; Ontology ; Robotics ; Robots},
language = {eng},
number = {2},
pages = {355-382},
publisher = {Wiley Subscription Services, Inc},
title = {Next‐generation human‐robot interaction with ChatGPT and robot operating system},
volume = {55},
year = {2025},
}

@article{YeYang2023ITiH,
abstract = {Human-robot collaboration is becoming increasingly important as robots become more involved in various aspects of human life in the era of Artificial Intelligence. However, the issue of human operators' trust in robots remains a significant concern, primarily due to the lack of adequate semantic understanding and communication between humans and robots. The emergence of Large Language Models (LLMs), such as ChatGPT, provides an opportunity to develop an interactive, communicative, and robust human-robot collaboration approach. This paper explores the impact of ChatGPT on trust in a human-robot collaboration assembly task. This study designs a robot control system called RoboGPT using ChatGPT to control a 7-degree-of-freedom robot arm to help human operators fetch, and place tools, while human operators can communicate with and control the robot arm using natural language. A human-subject experiment showed that incorporating ChatGPT in robots significantly increased trust in human-robot collaboration, which can be attributed to the robot's ability to communicate more effectively with humans. Furthermore, ChatGPT's ability to understand the nuances of human language and respond appropriately helps to build a more natural and intuitive human-robot interaction. The findings of this study have significant implications for the development of human-robot collaboration systems.},
author = {Ye, Yang and You, Hengxu and Du, Jing},
address = {Piscataway},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2023},
issn = {2169-3536},
journal = {IEEE access},
keywords = {Artificial intelligence ; Chatbots ; Cooperation ; Human engineering ; Human-robot interaction ; Natural Language Processing ; Reliability ; Robots ; Trust},
language = {eng},
pages = {1-1},
publisher = {IEEE},
title = {Improved Trust in Human-Robot Collaboration with ChatGPT},
volume = {11},
year = {2023},
}

@article{GaoNan2024GSGS,
abstract = {Gesture synthesis has gained significant attention as a critical research field, aiming to produce contextually appropriate and natural gestures corresponding to speech or textual input. Although deep learning-based approaches have achieved remarkable progress, they often overlook the rich semantic information present in the text, leading to less expressive and meaningful gestures. In this letter, we propose GesGPT, a novel approach to gesture generation that leverages the semantic analysis capabilities of large language models, such as ChatGPT. By capitalizing on the strengths of LLMs for text analysis, we adopt a controlled approach to generate and integrate professional gestures and base gestures through a text parsing script, resulting in diverse and meaningful gestures. Firstly, our approach involves the development of prompt principles that transform gesture generation into an intention classification problem using ChatGPT. We also conduct further analysis on emphasis words and semantic words to aid in gesture generation. Subsequently, we construct a specialized gesture lexicon with multiple semantic annotations, decoupling the synthesis of gestures into professional gestures and base gestures. Finally, we merge the professional gestures with base gestures. Experimental results demonstrate that GesGPT effectively generates contextually appropriate and expressive gestures.},
author = {Gao, Nan and Zhao, Zeyu and Zeng, Zhi and Zhang, Shuwu and Weng, Dongdong and Bao, Yihua},
address = {Piscataway},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2024},
issn = {2377-3766},
journal = {IEEE robotics and automation letters},
keywords = {Chatbots ; Cognition ; Human-robot interaction ; Machine learning ; Semantics ; Speech ; Synthesis ; Task analysis},
language = {eng},
number = {3},
pages = {2718-2725},
publisher = {IEEE},
title = {GesGPT: Speech Gesture Synthesis With Text Parsing From ChatGPT},
volume = {9},
year = {2024},
}

@article{FatahiSomayeh2024CeiC,
abstract = {Recent advances in generative Artificial Intelligence (AI) and Natural Language Processing (NLP) have led to the development of Large Language Models (LLMs) and AI-powered chatbots like ChatGPT, which have numerous practical applications. Notably, these models assist programmers with coding queries, debugging, solution suggestions, and providing guidance on software development tasks. Despite known issues with the accuracy of ChatGPT's responses, its comprehensive and articulate language continues to attract frequent use. This indicates potential for ChatGPT to support educators and serve as a virtual tutor for students.
To explore this potential, we conducted a comprehensive analysis comparing the emotional content in responses from ChatGPT and human answers to 2000 questions sourced from Stack Overflow (SO). The emotional aspects of the answers were examined to understand how the emotional tone of AI responses compares to that of human responses.
Our analysis revealed that ChatGPT's answers are generally more positive compared to human responses. In contrast, human answers often exhibit emotions such as anger and disgust. Significant differences were observed in emotional expressions between ChatGPT and human responses, particularly in the emotions of anger, disgust, and joy. Human responses displayed a broader emotional spectrum compared to ChatGPT, suggesting greater emotional variability among humans.
The findings highlight a distinct emotional divergence between ChatGPT and human responses, with ChatGPT exhibiting a more uniformly positive tone and humans displaying a wider range of emotions. This variance underscores the need for further research into the role of emotional content in AI and human interactions, particularly in educational contexts where emotional nuances can impact learning and communication.},
author = {Fatahi, Somayeh and Vassileva, Julita and Roy, Chanchal K.},
address = {Switzerland},
copyright = {Copyright © 2024 Fatahi, Vassileva and Roy.},
issn = {2624-8212},
journal = {Frontiers in artificial intelligence},
keywords = {Artificial intelligence ; Natural Language Processing},
language = {eng},
pages = {1393903-},
publisher = {Frontiers Media S.A},
title = {Comparing emotions in ChatGPT answers and human answers to the coding questions on Stack Overflow},
volume = {7},
year = {2024},
}

@article{YuWeiwei2025EHea,
abstract = {The deployment of robots in human‐centric environments has significantly increased in recent years. It is crucial for robots to navigate human environments while understanding social norms and personal boundaries to ensure a harmonious coexistence between humans and robots. A socially aware robot should be capable of interpreting and responding appropriately to human cues, expressions, and intentions, thereby fostering trust and confidence among humans. However, prior studies were insufficient or unable to address the navigation challenges in human‐populated environments, as they perceive people as obstacles rather than social agents. Recent studies have utilized proxemic areas that are present in interpersonal interactions for human‐robot interaction scenarios, but they have enforced consistent proxemic areas for social robot navigation. This approach fails to fully capture the highly sophisticated behaviour and preferences of humans. Therefore, we propose a psychologically‐based adaptive proxemic area that fluctuates based on the human's emotional state. Furthermore, we integrate this feature into a reinforcement learning‐based social navigation framework, making our navigation framework robust to the unpredictable affections of humans. Additionally, our navigation framework includes human intention prediction to approximate the future proxemic area, thereby avoiding interference with the path to be taken by individuals. We have named our framework the Human Emotion and Intention Aware Path Planner (EmoiPlanner). Our framework has been subjected to case studies involving realistic crowd navigation scenarios, and the results indicate that it enables robots to navigate through crowds without causing discomfort to pedestrians who exhibit stochastic behaviours and emotional states, while also ensuring efficient path planning.},
author = {Yu, Weiwei and Kok, Siong Yuen and Srivastava, Gautam},
address = {Oxford},
copyright = {2024 John Wiley & Sons Ltd.},
issn = {0266-4720},
journal = {Expert systems},
keywords = {Emotions ; Machine learning ; Navigation ; Pedestrians ; Reinforcement learning ; Robots},
language = {eng},
number = {2},
publisher = {Blackwell Publishing Ltd},
title = {EmoiPlanner: Human emotion and intention aware socially acceptable robot navigation in human‐centric environments},
volume = {42},
year = {2025},
}

@article{HoManh-Tung2023Utao,
abstract = {Despite having one of the most advanced healthcare systems in the world, Japan is expected to experience a shortage of nearly half a million healthcare workers by 2025 due to its rapidly aging population. In response, government authorities plan to implement a wide range of AI-driven healthcare solutions. These include care robots that assist the physically handicapped or elderly, chatbots that provide anonymous online mental health consultation, and diagnostic software utilizing machine learning. Yet one of the most popular smart technologies to augment the nation's already overstrained and undermanned healthcare system is a little known but emerging emotional AI technologies, i.e., deep learning systems trained to read, classify, and respond to human emotions. These technologies are being sold on a commercial level not only to the public but also to rehabilitation centers, local hospitals, and senior citizen residences. Although the augmentation of healthcare services to intelligent machines may seem like a logical step in a country well-known for its long-standing affection toward robots, Japanese society is also known for its adherence to established social relations and traditional institutional practices, especially, in the realm of medical care. In order to gauge Japanese acceptance of emotion-sensing technology, we analyze a dataset of 245 visitors to clinics and hospitals in a typical suburban area in Japan using multiple linear regression. The results show that in general, senior and male patients perceive the emotional AI technology more negatively. For behavioral variables, patients' level of familiarity has positive correlations with attitudes toward emotional AI-based applications in private setting (βFamiliarity_AttitudePri=0.346, p<0.001) and public setting (βFamiliarity_PublicAttitude=0.297, p<0.001); while concern for losing control to AI has negative correlations with the attitudes' variables: private setting (βLosingControl_AttitudePri=−0.262,p=0.002) and public setting (β LosingControl_AttitudePub=-0.188, p=0.044). Interestingly, concerns over violation of privacy and discrimination are non-significant correlates, which contradict the emerging literature on this subject. We further contextualize the findings with insights afforded by an understanding of Japanese culture as well as the relevant literature on care robots in Japan. Finally, policy and education implications to promote emotional AI acceptance to the general and senior members of the society are provided.
•Attitude toward emotional AI use in Japanese healthcare positively correlates with familiarity with the technology.•Fear of losing control to AI has significant negative correlation to the perception of EAI in both private and public setting.•Privacy and discriminatory concerns are non-significant correlates of the attitude toward emotional AI use in healthcare.•The results necessitate the cultural framing of technological acceptance behaviors.},
author = {Ho, Manh-Tung and Le, Ngoc-Thang B. and Mantello, Peter and Ho, Manh-Toan and Ghotbi, Nader},
copyright = {2022 Elsevier Ltd},
issn = {0160-791X},
journal = {Technology in society},
keywords = {Japan},
language = {eng},
pages = {102166-},
publisher = {Elsevier Ltd},
title = {Understanding the acceptance of emotional artificial intelligence in Japanese healthcare system: A cross-sectional survey of clinic visitors’ attitude},
volume = {72},
year = {2023},
}

@article{SharmaVarsha2024KaAo,
abstract = {ABSTRACT
Background:
One of the most crucial steps in providing the child's oral health care and treatment needs is modifying his/her behavior, mood and emotion in any primary care/dental home or any speciality centre. This is very important parameters for any dental professionals and paediatric dentist. Through this study we investigated the knowledge, awareness of Emotional Artificial Intelligence (AI) as tool for modifying their behaviour prior to any treatment requirement.
Materials and Methods:
The study was conducted among dental professionals of located in different regions of eastern India. We used a consecutive sampling method to collect our sample.
Results:
Out of the 120 participating dental professionals, 30% professionals believed that emotional AI could monitor a child's acceptance to treatment. Despite a low perception level of emotional AI by the dentist, 71% agreed on the beneficial effect of Emotional Artificial Intelligence (AI) in an emergency or uncooperative child if such technology exist.
Conclusion:
Regression model showed that certain key influencers can increase the quality of healthcare by 3 times which could favors Emotional Artificial Intelligence as an adjunct to Pediatric dentists in managing children.},
author = {Sharma, Varsha and Sharma, Mukul and Dutta, Brahmananda and Bagchi, Anandamoy},
address = {India},
copyright = {Copyright: © 2024 Journal of Pharmacy and Bioallied Sciences},
edition = {2},
issn = {0976-4879},
journal = {Journal of pharmacy \& bioallied science},
keywords = {Artificial intelligence ; Awareness ; Cross-Sectional Studies ; Dental hygiene ; Dentists ; Pediatrics ; Perception ; Primary health care},
language = {eng},
number = {Suppl 3},
pages = {S2033-S2035},
publisher = {Wolters Kluwer - Medknow},
title = {Knowledge and Awareness of Emotional Artificial Intelligence as Tool in Child's Oral Health Care Assessed Among Dental Professionals of Eastern India. A Cross Sectional Study},
volume = {16},
year = {2024},
}

@article{AbdollahiHojjat2023AEIi,
abstract = {This paper presents our recent research on integrating artificial emotional intelligence in a social robot (Ryan) and studies the robot's effectiveness in engaging older adults. Ryan is a socially assistive robot designed to provide companionship for older adults with depression and dementia through conversation. We used two versions of Ryan for our study, empathic and non-empathic. The empathic Ryan utilizes a multimodal emotion recognition algorithm and a multimodal emotion expression system. Using different input modalities for emotion, i.e., facial expression and speech sentiment, the empathic Ryan detects users' emotional state and utilizes an affective dialogue manager to generate a response. On the other hand, the non-empathic Ryan lacks facial expression and uses scripted dialogues that do not factor in the users' emotional state. We studied these two versions of Ryan with 10 older adults living in a senior care facility. The statistically significant improvement in the users' reported face-scale mood measurement indicates an overall positive effect from the interaction with both the empathic and non-empathic versions of Ryan. However, the number of spoken words measurement and the exit survey analysis suggest that the users perceive the empathic Ryan as more engaging and likable.},
author = {Abdollahi, Hojjat and Mahoor, Mohammad H. and Zandie, Rohola and Siewierski, Jarid and Qualls, Sara H.},
address = {United States},
copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2023},
issn = {1949-3045},
journal = {IEEE transactions on affective computing},
keywords = {Adulthood ; Algorithms ; Dementia ; Depression Mental ; Emotion recognition ; Emotional intelligence ; Emotions ; Older people ; Robots ; Sentiment analysis},
language = {eng},
number = {3},
pages = {2020-2032},
publisher = {IEEE},
title = {Artificial Emotional Intelligence in Socially Assistive Robots for Older Adults: A Pilot Study},
volume = {14},
year = {2023},
}

--------------------------------------------------------------------------------
@article{GasteigerNorina2021Fftf,
copyright = {Copyright 2021 Elsevier B.V., All rights reserved.},
issn = {1176-9092},
journal = {Clinical interventions in aging},
keywords = {Adaptability (Psychology) ; Alzheimer's disease ; Cognition in animals ; Computers ; COVID-19 Pandemic 2020- ; Feasibility studies ; Health aspects ; Human beings ; Immune system ; Interpersonal relations ; Loneliness ; Male ; Older people ; Oral communication ; Psychological aspects ; Review ; Robots ; Social aspects ; Social interaction ; Social networks ; Social skills},
language = {eng},
pages = {941-971},
publisher = {Dove Medical Press Ltd},
abstract = {Background and Aim: Loneliness is a common problem in older adults and contributes to poor health. This scoping review aimed to synthesize and report evidence on the effectiveness of interventions using social robots or computer agents to reduce loneliness in older adults and to explore intervention strategies.
Methods: The review adhered to the Arksey and O'Malley process for conducting scoping reviews. The SCOPUS, PUBMED, Web of Science, EMBASE, CINAHL, PsycINFO, ACM Digital Library and IEEE Xplore databases were searched in November, 2020. A two-step selection process identified eligible research. Information was extracted from papers and entered into an Excel coding sheet and summarised. Quality assessments were conducted using the Mixed Methods Appraisal Tool.
Results: Twenty-nine studies were included, of which most were of moderate to high quality. Eighteen were observational and 11 were experimental. Twenty-four used robots, four used computer agents and one study used both. The majority of results showed that robots or computer agents positively impacted at least one loneliness outcome measure. Some unintended negative consequences on social outcomes were reported, such as sadness when the robot was removed. Overall, the interventions helped to combat loneliness by acting as a direct companion (69%), a catalyst for social interaction (41%), facilitating remote communication with others (10%) and reminding users of upcoming social engagements (3%).
Conclusion: Evidence to date suggests that robots can help combat loneliness in older adults, but there is insufficient research on computer agents. Common strategies for reducing loneliness include direct companionship and enabling social interactions. Future research could investigate other strategies used in human interventions (eg, addressing maladaptive social cognition and improving social skills), and the effects of design features on efficacy. It is recommended that more robust experimental and mixed methods research be conducted, using a combination of validated self-report, observational, and interview measures of loneliness.},
author = {Gasteiger, Norina and Loveys, Kate and Law, Mikaela and Broadbent, Elizabeth},
address = {ALBANY},
title = {Friends from the future: A scoping review of research into robots and computer agents to combat loneliness in older people},
volume = {16},
year = {2021},
}

@article{DossoJillA.2023“SSf,
copyright = {Copyright 2024 Elsevier B.V., All rights reserved.},
issn = {2414-4088},
journal = {Multimodal technologies and interaction},
keywords = {Adulthood ; Anxiety ; Anxiety disorders ; Autism ; Breathing exercises ; Child psychology ; Children ; Consultants ; Emotion recognition ; Emotions ; Family ; Hospitals ; Interpersonal relations ; Mental health ; Mental illness ; Pediatrics ; Psychology Pathological ; Research ; Robotics ; Robots ; Social networks ; Workshops},
language = {eng},
number = {12},
pages = {118-},
publisher = {MDPI AG},
abstract = {Social robots have the potential to support health and quality of life for children experiencing anxiety. We engaged families with lived experiences of pediatric anxiety in social robot development to explore desired design features, application areas, and emotion functionalities of social robots in anxiety care. We conducted 10 online co-creation workshops with (1) children with anxiety aged 7–13 (n = 24) with their family members (n = 20), and (2) youth with anxiety aged 14–18 (n = 12). Workshop participation included a validated robot expectations scale, anonymous polls, and discussion. Transcripts and text responses were subjected to content analysis. A lived experience expert group provided feedback throughout the research. Participants desired a pet-like robot with a soft texture, expressive eyes, and emotion detection to support activities of daily living. Specific anxiety-related applications included breathing exercises, managing distressing thoughts, and encouragement. Emotional alignment, the design of a robot’s emotional display, and the emotional impacts of an interaction were discussed. Privacy and the replacement of human interaction were concerns. We identify pediatric anxiety-specific design features, applications, and affective considerations for existing and future social robots. Our findings highlight the need for customizability and robust emotional functionality in social robot technologies intended to support the health and care of children living with anxiety.},
author = {Dosso, Jill A. and Kailley, Jaya N. and Martin, Susanna E. and Robillard, Julie M.},
address = {Basel},
title = {“A Safe Space for Sharing Feelings”: Perspectives of Children with Lived Experiences of Anxiety on Social Robots},
volume = {7},
year = {2023},
}

@article{DossoJillA2022Teas,
copyright = {2022 the Alzheimer's Association.},
issn = {1552-5260},
journal = {Alzheimer's and dementia},
keywords = {Canada ; Dementia ; Human beings ; Independent Living ; Interpersonal relations ; Older people ; Robotics ; Social interaction},
language = {eng},
number = {S2},
pages = {e059261-n/a},
abstract = {Background
Persons living with dementia and their care partners place a high value on aging in place and maintaining independence. Socially assistive robots – embodied characters or pets that provide companionship and aid through social interaction – are a promising tool to support these goals. There is a growing commercial market for these devices, with functions including medication reminders, conversation, pet‐like behaviours, and even the collection of health data. While potential users generally report positive feelings towards social robots, persons with dementia have been under‐included in design and development, leading to a disconnect between robot functions and the real‐world needs and desires of end‐users. Furthermore, a key element of social and emotional connectedness in human relationships is emotional alignment – a state where all partners have congruent emotional understandings of a situation. Strong emotional alignment between users and robots will be necessary for social robots to provide meaningful companionship, but a computational model of how to achieve this has been absent from the field. To this end, we propose and test Affect Control Theory (ACT) as a framework to improve emotional alignment between older adults and social robotics.
Method
Using a Canadian online survey, we introduced respondents to three exemplar social robots with older adult‐specific functionalities and evaluated their responses around features, emotions, and ethics using standardized and novel measures (n=171 older adults, n=28 care partners, and n=7 persons living with dementia).
Result
Overall, participants responded positively to the robots. High priority uses included companionship, interaction, and safety. Reasoning around robot use was pragmatic; curiosity and entertainment were motivators to use, while a perceived lack of need and the mechanical appearance of the robots were detractors. Realistic, cute, and cuddly robots were preferred while artificial‐looking, creepy, and toy‐like robots were disliked. Most importantly, our evidence supported ACT as a viable model of human‐robot emotional alignment.
Conclusion
This work supports the development of emotionally sophisticated, evidence‐based, and user‐centered social robotics with older adult‐ and dementia‐specific functionality.},
author = {Dosso, Jill A and Bandari, Ela and Malhotra, Aarti and Hoey, Jesse and Michaud, Francois and Prescott, Tony J and Robillard, Julie M.},
address = {United States},
title = {Towards emotionally aligned social robots for dementia: perspectives of care partners and persons with dementia},
volume = {18},
year = {2022},
}

@article{XieQisi2023THEF,
copyright = {Copyright 2024 Elsevier B.V., All rights reserved.},
issn = {2158-107X},
journal = {International journal of advanced computer science and applications},
keywords = {Animation ; Gestalt Theory ; Human-robot interaction ; Robots},
language = {eng},
number = {10},
pages = {642-651},
publisher = {Science and Information (SAI) Organization Limited},
abstract = {The development of technology and the increasing prevalence of solitary living have transformed non-humanoid robots, such as robotic sweepers and mechanical pets, into potential sources of emotional support for individuals. Nevertheless, the majority of non-humanoid robots currently in existence are task-oriented and lack features such as facial expressions and sound. Existing research primarily emphasizes the details of human motion in robot motion design, while devoting less attention to the analysis of universal emotional expression factors and methods rooted in human recognition patterns. In our initial step, a theoretical framework and holistic expression factors were proposed based on Gestalt theory and SOR theory. These factors encompass vertical and horizontal motion direction, stimulation, and vertical repetition. Subsequently, animation simulation tests were conducted to confirm and examine the contributions of each factor to the recognition of emotional expressions. The results indicate that both vertical and horizontal movements can convey emotional valence. However, if both of them exist, there is no leading direction to the valence recognition result. When both vertical and horizontal movements are present, valence recognition is influenced by the combined effects of stimulation, vertical repetition, and movement direction. Simultaneously, non-humanoid robots can display recognizable emotional content when influenced by holistic expression factors. This framework can serve as a universal guide for emotional expression tasks in non-humanoid robots, proving the hypothesis that Gestalt theory is applicable in dynamic emotional recognition tasks. At the same time, these findings propose a new holistic perspective for designing emotional expression methods for robots.},
author = {Xie, Qisi and Luh, Ding-Bang},
address = {West Yorkshire},
title = {The Holistic Expression Factors of Emotional Motion for Non-humanoid Robots},
volume = {14},
year = {2023},
}

@article{HakimVandoGustiAl2024DToP,
copyright = {Copyright 2024 Elsevier B.V., All rights reserved.},
issn = {1939-1382},
journal = {IEEE transactions on learning technologies},
keywords = {College students ; Communication ; Computer science ; Education ; Journalism Educational ; Robotics ; Robots ; Social sciences ; Students ; Task analysis ; Technology ; Virtual reality},
language = {eng},
pages = {1843-1857},
publisher = {IEEE},
abstract = {The use of robots in education has the potential to engage students in learning activities and aims to form lasting relationships with them. To encourage sustainable, long-term human-robot interactions, a promising approach is to cultivate a pet-like, interdependent relationship. However, the potential of such relationships in education remains unclear, and the limited availability of robots in classrooms necessitates flexible and scalable designs. To address these challenges, this study leverages digital twin technology to facilitate ubiquitous engagement with pet robots, thereby prolonging interdependent relationships through a SeamlessPet robot learning approach. Here, students engaged with both virtual and physical pet robots, enabling realistic and continuous interactions akin to communicating directly with a physical robot. This integration ensured consistent availability and authentic interactions, enhancing educational outcomes demonstrated in situational presentations. An experiment with 70 university students in a Japanese Hospitality Management Program in Taiwan demonstrated that this approach resulted in better learning achievements and fostered a positive learning experience. The pet-like features embedded within the digital twin robots played a vital role in fostering prolonged learning participation, empowering students to take ownership of their learning, stay motivated, and feel supported at any time and from anywhere in the learning process. Educators and curriculum developers are encouraged to consider this approach, particularly in courses with a final project presentation that uses a robot to demonstrate study results.},
author = {Hakim, Vando Gusti Al and Yang, Su-Hang and Wang, Jen-Hang and Lin, Hung-Hsuan and Chen, Gwo-Dong},
address = {LOS ALAMITOS},
title = {Digital Twins of Pet Robots to Prolong Interdependent Relationships and Effects on Student Learning Performance},
volume = {17},
year = {2024},
}

@article{PaluchRichard2022"SfC,
copyright = {ACM},
issn = {2573-0142},
journal = {Proceedings of the ACM on human-computer interaction},
language = {eng},
number = {GROUP},
pages = {1-35},
publisher = {ACM},
abstract = {Robotic systems are increasingly seen as possible technical aids against the background of demographic change and the associated pressures on care systems, with increasing numbers of care recipients and a decreasing number of trained caregivers. In human-computer interaction and computer-supported cooperative work, different design paradigms are currently being pursued to explore which features and appearances are favorable for meaningful interactions of humans with robotic systems. One such approach, labeled as "otherware", proposes to conceptualize robots beyond a naive anthropomorphism or zoomorphism, rather developing the idea of a figure that goes beyond the dichotomy between "being alive" and "being a technical artefact". We present an ethnographic study on the perceptions, attitudes, and practices of care attendants and nursing-home residents in their experimenting with off-the-shelf robotic cats and dogs. The three-week study shows specific appropriation practices of the robotic pets, and how the care attendants - partly together with the residents - define their experiences of the robotic pets, i.e., in which situations the robotic pets are considered either as living beings or as technology toys. The study provides practice-based insights into how possible uses of robotic pets could be meaningfully integrated into care practices, but also which ethical reflections were discussed during their use. Finally, this ethnographic study functioned as a collaborative learning process between researchers, care attendants, and residents, and thus also points out possible aspects that arose with regard to future learning spaces of professional and organizational development for dealing with innovative technologies in residential care contexts.},
author = {Paluch, Richard and Müller, Claudia},
address = {New York, NY, USA},
title = {"That's Something for Children": An Ethnographic Study of Attitudes and Practices of Care Attendants and Nursing Home Residents Towards Robotic Pets},
volume = {6},
year = {2022},
}

@article{RiddochKatieA2022Ebpa,
copyright = {Copyright 2022 Elsevier B.V., All rights reserved.},
issn = {1932-6203},
journal = {PloS one},
keywords = {Animal behavior ; Behavior ; Bonding ; Dementia ; Dogs ; Ethics ; Human acts ; Human behavior ; Loneliness ; Mental health ; Pets ; Psychological aspects ; Robotics ; Robots ; Social aspects ; Social sciences},
language = {eng},
number = {9},
pages = {e0274353-e0274353},
publisher = {Public Library of Science},
abstract = {To facilitate long-term engagement with social robots, emerging evidence suggests that modelling robots on social animals with whom many people form enduring social bonds–specifically, pet dogs–may be useful. However, scientific understanding of the features of pet dogs that are important for establishing and maintaining social bonds remains limited to broad qualities that are liked, as opposed to specific behaviours. To better understand dog behaviours that are perceived as important for facilitating social bonds between owner and pet, we surveyed current dog owners (n = 153) with open-ended questions about their dogs’ behaviours. Thematic analysis identified 7 categories of behaviours perceived as important to human—dog bonding, including: 1) attunement, 2) communication, 3) consistency and predictability, 4) physical affection, 5) positivity and enthusiasm, 6) proximity, and 7) shared activities. We consider the feasibility of translating these behaviours into a social robotic platform, and signpost potential barriers moving forward. In addition to providing insight into important behaviours for human—dog bonding, this work provides a springboard for those hoping to implement dog behaviours into animal-like artificial agents designed for social roles.},
author = {Riddoch, Katie A and Hawkins, Roxanne D and Cross, Emily S},
address = {San Francisco},
title = {Exploring behaviours perceived as important for human—Dog bonding and their translation to a robotic platform},
volume = {17},
year = {2022},
}

@article{BharatharajJaishankar2022TPUW,
copyright = {2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2313-7673},
journal = {Biomimetics (Basel, Switzerland)},
keywords = {Cell phones ; Classification ; Communication ; Design ; Education ; Pedestrians ; Robotics ; Robots ; Vision disorders ; Visual perception ; Walking},
language = {eng},
number = {2},
pages = {81-},
publisher = {MDPI AG},
abstract = {Research indicates that deaths due to fall incidents are the second leading cause of unintentional injury deaths in the world. Death by fall due to a person texting or talking on mobile phones while walking, impaired vision, unexpected terrain changes, low balance, weakness, and chronic conditions has increased drastically over the past few decades. Particularly, unexpected terrain changes would many times lead to severe injuries and sometimes death even in healthy individuals. To tackle this problem, a warning system to alert the person of the imminent danger of a fall can be developed. This paper describes a solution for such a warning system used in our bio-inspired wearable pet robot, KiliRo. It is a terrain perception system used to classify the terrain based on visual features obtained from processing the images captured by a camera and notify the wearer of terrain changes while walking. The parrot-inspired KiliRo robot can twist its head and the camera up to 180 degrees to obtain visual feedback for classification. Feature extraction is followed by K-nearest neighbor for terrain classification. Experiments were conducted to establish the efficacy and validity of the proposed approach in classifying terrain changes. The results indicate an accuracy of over 95% across five terrain types, namely pedestrian pathway, road, grass, interior, and staircase.},
author = {Bharatharaj, Jaishankar and Huang, Loulin and Al-Jumaily, Ahmed M. and Kutty, Senthil Kumar Sasthan and Krägeloh, Chris},
address = {Basel},
title = {Terrain Perception Using Wearable Parrot-Inspired Companion Robot, KiliRo},
volume = {7},
year = {2022},
}

================================================================================
